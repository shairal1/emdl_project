# Understanding Inherited Bias in Data Pipelines
When constructing data pipelines for machine learning models, it is crucial to prevent inherited bias. Inherited bias occurs when machine learning algorithms use outputs generated by other ML-based tools as input, thereby inheriting any preexisting biases present in those tools. This perpetuates and possibly amplifies existing biases, leading to unfair or biased outcomes. In this guide, we will explain the impact of inherited bias, how it occurs, and practical steps to mitigate it.

## The Problem: Inherited Bias
Inherited bias arises when downstream models use features or labels produced by upstream models that contain biases. These biases can result from various sources, such as biased training data, flawed algorithms, or inappropriate feature selection. When these biased outputs are used as inputs for new models, they propagate the biases, leading to unfair predictions and decisions. This can be particularly problematic in sensitive applications like hiring, lending, and law enforcement.

## Practical Advice

1. **Bias Detection and Mitigation**: Before using outputs from other ML tools or unknown sources of data, they should be assessed for biases and mitigate techniques should be applied for any identified biases.

2. **Transparent Models**: Interpretable models should be used to understand how inherited features influence predictions and identify potential sources of bias.

3. **Bias-Aware Training**: If significant performance disparities are found, separate models or different preprocessing steps for each slice should be considered. This can help tailor the model to perform better on specific subgroups.

4. **Monitoring and Iteration Should Be Continuous**: Fairness-aware algorithms and techniques should be incorporated during model training to address and reduce inherited biases.

5. **Evaluation Metrics**: Fairness metrics could be used in addition to traditional performance metrics to evaluate model performance and ensure fairness.

## References
1. Hellstr√∂m, T., Dignum, V. and Bensch, S., 2020. Bias in Machine Learning--What is it Good for?. arXiv preprint arXiv:2004.00686.